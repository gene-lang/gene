## Gene VM bindings for OpenAI API
## Bridges the Nim OpenAI client to Gene's VM system

import tables, json, strutils
import ../../gene/types
import ../../gene/vm
import openai_client, streaming

var openai_client_class*: Class
var openai_error_class*: Class
var openai_clients: Table[system.int64, OpenAIConfig] = initTable[system.int64, OpenAIConfig]()
var next_client_id: system.int64 = 1

proc openai_client_constructor(vm: VirtualMachine, args: ptr UncheckedArray[Value], arg_count: int, has_keyword_args: bool): Value {.gcsafe.}

# Helper to convert Gene Value to JsonNode
proc geneValueToJson*(value: Value): JsonNode =
  case value.kind
  of VkNil:
    result = newJNull()
  of VkBool:
    result = %*value.to_bool
  of VkInt:
    result = %*value.int
  of VkFloat:
    result = %*value.float
  of VkString:
    result = %*value.str
  of VkArray:
    var arr = newJArray()
    for item in value.ref.arr:
      arr.add(geneValueToJson(item))
    result = arr
  of VkMap:
    var obj = newJObject()
    for key, val in value.ref.map:
      let symbol_value = cast[Value](key)
      obj[symbol_value.str] = geneValueToJson(val)
    result = obj
  of VkGene:
    # Handle Gene expressions by evaluating them first
    # For now, convert to string
    result = %*($value)
  else:
    result = %*($value)

# Helper to convert JsonNode to Gene Value
proc jsonToGeneValue*(json: JsonNode): Value =
  case json.kind
  of JNull:
    result = NIL
  of JBool:
    result = json.getBool.to_value
  of JInt:
    result = json.getInt.to_value
  of JFloat:
    result = json.getFloat.to_value
  of JString:
    result = json.getStr.to_value
  of JArray:
    var arr = newSeq[Value]()
    for item in json:
      arr.add(jsonToGeneValue(item))
    result = new_array_value(arr)
  of JObject:
    var map = initTable[Key, Value]()
    for key, value in json:
      map[key.to_key()] = jsonToGeneValue(value)
    result = new_map_value(map)

# Helper to create error objects
proc attach_error_class(instance: ptr Reference) {.gcsafe.} =
  {.cast(gcsafe).}:
    if not openai_error_class.isNil:
      instance.instance_class = openai_error_class

proc new_error*(message: string): Value {.gcsafe.} =
  var error_obj = new_ref(VkInstance)
  attach_error_class(error_obj)
  error_obj.instance_props = initTable[Key, Value]()
  error_obj.instance_props["message".to_key()] = message.to_value
  error_obj.instance_props["type".to_key()] = "error".to_value
  result = error_obj.to_ref_value()

proc openai_error_value*(err: OpenAIError): Value {.gcsafe.} =
  var error_obj = new_ref(VkInstance)
  attach_error_class(error_obj)
  error_obj.instance_props = initTable[Key, Value]()
  error_obj.instance_props["message".to_key()] = err.msg.to_value
  error_obj.instance_props["status".to_key()] = err.status.to_value
  if err.provider_error.len > 0:
    error_obj.instance_props["provider_error".to_key()] = err.provider_error.to_value
  if err.request_id.len > 0:
    error_obj.instance_props["request_id".to_key()] = err.request_id.to_value
  if err.retry_after != 0:
    error_obj.instance_props["retry_after".to_key()] = err.retry_after.to_value
  if err.metadata != nil:
    error_obj.instance_props["metadata".to_key()] = jsonToGeneValue(err.metadata)
  result = error_obj.to_ref_value()

proc register_client(config: OpenAIConfig): Value =
  let client_id = next_client_id
  inc(next_client_id)
  openai_clients[client_id] = config

  let instance = new_ref(VkInstance)

  # Try to use the global class first, but fall back to creating a new one if needed
  {.cast(gcsafe).}:
    if openai_client_class != nil:
      instance.instance_class = openai_client_class
    else:
      # Create a temporary class if the global one is not yet available
      var base_parent: Class = nil
      if App != nil and App.app.object_class.kind == VkClass:
        base_parent = App.app.object_class.ref.class
      instance.instance_class = new_class("OpenAIClient", base_parent)

  instance.instance_props = initTable[Key, Value]()
  instance.instance_props["client_id".to_key()] = client_id.to_value
  instance.instance_props["base_url".to_key()] = config.base_url.to_value
  instance.instance_props["model".to_key()] = config.model.to_value

  result = instance.to_ref_value()

proc fetch_client_config(client_val: Value): tuple[config: OpenAIConfig, err: Value] =
  if client_val.kind != VkInstance:
    return (nil, new_error("Invalid OpenAI client"))

  if not client_val.ref.instance_props.has_key("client_id".to_key()):
    return (nil, new_error("Invalid OpenAI client"))

  let client_id = client_val.ref.instance_props["client_id".to_key()].to_int()
  var cfg: OpenAIConfig = nil
  {.cast(gcsafe).}:
    if openai_clients.hasKey(client_id):
      cfg = openai_clients[client_id]

  if cfg.isNil:
    return (nil, new_error("OpenAI client not found"))

  (cfg, NIL)

# Native function: Create new OpenAI client
proc vm_openai_new_client*(vm: VirtualMachine, args: ptr UncheckedArray[Value], arg_count: int, has_keyword_args: bool): Value =
  var options: JsonNode = newJNull()

  if get_positional_count(arg_count, has_keyword_args) > 0:
    let options_val = get_positional_arg(args, 0, has_keyword_args)
    options = geneValueToJson(options_val)

  let config = buildOpenAIConfig(options)
  return register_client(config)

proc openai_client_constructor(vm: VirtualMachine, args: ptr UncheckedArray[Value], arg_count: int, has_keyword_args: bool): Value {.gcsafe.} =
  var options: JsonNode = newJNull()
  if get_positional_count(arg_count, has_keyword_args) > 0:
    options = geneValueToJson(get_positional_arg(args, 0, has_keyword_args))

  let config = buildOpenAIConfig(options)
  {.cast(gcsafe).}:
    result = register_client(config)

proc openai_error_result(err: OpenAIError): Value =
  return openai_error_value(err)

proc call_openai_endpoint(config: OpenAIConfig, endpoint: string, payload: JsonNode): Value {.gcsafe.} =
  try:
    let response = performRequest(config, "POST", endpoint, payload)
    return jsonToGeneValue(response)
  except OpenAIError as e:
    return openai_error_value(e)

proc call_gene_callable(vm: VirtualMachine, callable: Value, args: seq[Value]) {.gcsafe.} =
  case callable.kind
  of VkNativeFn:
    discard call_native_fn(callable.ref.native_fn, vm, args)
  of VkFunction:
    {.cast(gcsafe).}:
      discard vm.exec_function(callable, args)
  of VkClass:
    if callable.ref.class.methods.hasKey("call".to_key()):
      let call_method = callable.ref.class.methods["call".to_key()].callable
      var new_args = @[callable]
      new_args.add(args)
      call_gene_callable(vm, call_method, new_args)
  of VkInstance:
    let inst = callable.ref
    if not inst.instance_class.isNil and inst.instance_class.methods.hasKey("call".to_key()):
      let call_method = inst.instance_class.methods["call".to_key()].callable
      var new_args = @[callable]
      new_args.add(args)
      call_gene_callable(vm, call_method, new_args)
  else:
    discard

proc createGeneStreamHandler(vm: VirtualMachine, callback: Value): StreamHandler =
  proc handler(event: StreamEvent) {.gcsafe.} =
    try:
      var map = initTable[Key, Value]()
      map["event".to_key()] = event.event.to_value
      map["done".to_key()] = event.done.to_value
      if event.data != nil:
        map["data".to_key()] = jsonToGeneValue(event.data)
      else:
        map["data".to_key()] = NIL
      let event_value = new_map_value(map)
      call_gene_callable(vm, callback, @[event_value])
    except system.Exception as e:
      when defined(debug):
        echo "DEBUG: Stream handler error: ", e.msg
  return handler

proc openai_error_to_s(vm: VirtualMachine, args: ptr UncheckedArray[Value], arg_count: int, has_keyword_args: bool): Value =
  if get_positional_count(arg_count, has_keyword_args) < 1:
    return "OpenAIError".to_value()

  let self_val = get_positional_arg(args, 0, has_keyword_args)
  if self_val.kind != VkInstance:
    return "OpenAIError".to_value()

  let props = self_val.ref.instance_props
  let message_key = "message".to_key()
  var desc = "OpenAIError"
  if props.hasKey(message_key) and props[message_key].kind == VkString:
    desc &= ": " & props[message_key].str

  var details: seq[string] = @[]
  for key_name in ["status", "request_id", "provider_error"]:
    let k = key_name.to_key()
    if props.hasKey(k):
      details.add(key_name & "=" & $props[k])
  if props.hasKey("retry_after".to_key()):
    details.add("retry_after=" & $props["retry_after".to_key()])

  if details.len > 0:
    desc &= " (" & details.join(", ") & ")"

  return desc.to_value()

proc start_openai_stream(vm: VirtualMachine, config: OpenAIConfig, options: JsonNode, handler: Value): Value =
  if handler.kind notin {VkNativeFn, VkFunction, VkClass, VkInstance}:
    return new_error("Callback must be callable")

  var stream_opts = if options.kind == JNull: %*{} else: options
  stream_opts["stream"] = %*true
  let payload = buildChatPayload(config, stream_opts)

  let future_val = new_future_value()
  let future_obj = future_val.ref.future

  try:
    let stream_handler = createGeneStreamHandler(vm, handler)
    performStreamingRequest(config, "/chat/completions", payload, stream_handler)
    future_obj.complete("streaming completed".to_value)
  except OpenAIError as e:
    future_obj.fail(openai_error_value(e))
  except system.Exception as e:
    future_obj.fail(new_error("OpenAI stream failed: " & e.msg))

  return future_val

# Native function: Chat completion
proc vm_openai_chat*(vm: VirtualMachine, args: ptr UncheckedArray[Value], arg_count: int, has_keyword_args: bool): Value {.gcsafe.} =
  if get_positional_count(arg_count, has_keyword_args) < 2:
    return new_error("OpenAI chat requires client and options arguments")

  let client_val = get_positional_arg(args, 0, has_keyword_args)
  let options_val = get_positional_arg(args, 1, has_keyword_args)

  let (config, err) = fetch_client_config(client_val)
  if err != NIL:
    return err

  let options = geneValueToJson(options_val)
  let payload = buildChatPayload(config, options)
  return call_openai_endpoint(config, "/chat/completions", payload)

# Native function: Embeddings
proc vm_openai_embeddings*(vm: VirtualMachine, args: ptr UncheckedArray[Value], arg_count: int, has_keyword_args: bool): Value {.gcsafe.} =
  if get_positional_count(arg_count, has_keyword_args) < 2:
    return new_error("OpenAI embeddings requires client and options arguments")

  let client_val = get_positional_arg(args, 0, has_keyword_args)
  let options_val = get_positional_arg(args, 1, has_keyword_args)

  let (config, err) = fetch_client_config(client_val)
  if err != NIL:
    return err

  let options = geneValueToJson(options_val)
  let payload = buildEmbeddingsPayload(config, options)
  return call_openai_endpoint(config, "/embeddings", payload)

# Native function: Responses (for structured outputs)
proc vm_openai_respond*(vm: VirtualMachine, args: ptr UncheckedArray[Value], arg_count: int, has_keyword_args: bool): Value {.gcsafe.} =
  if get_positional_count(arg_count, has_keyword_args) < 2:
    return new_error("OpenAI respond requires client and options arguments")

  let client_val = get_positional_arg(args, 0, has_keyword_args)
  let options_val = get_positional_arg(args, 1, has_keyword_args)

  let (config, err) = fetch_client_config(client_val)
  if err != NIL:
    return err

  let options = geneValueToJson(options_val)
  let payload = buildResponsesPayload(config, options)
  return call_openai_endpoint(config, "/responses", payload)

# Native function: Stream chat completion (instance method)
proc vm_openai_client_stream*(vm: VirtualMachine, args: ptr UncheckedArray[Value], arg_count: int, has_keyword_args: bool): Value {.gcsafe.} =
  if get_positional_count(arg_count, has_keyword_args) < 2:
    return new_error("OpenAI client stream requires self and callback arguments")

  let self_val = get_positional_arg(args, 0, has_keyword_args)
  let callback_val = get_positional_arg(args, 1, has_keyword_args)

  let (config, err) = fetch_client_config(self_val)
  if err != NIL:
    return err

  var options = %*{}
  if get_positional_count(arg_count, has_keyword_args) > 2:
    options = geneValueToJson(get_positional_arg(args, 2, has_keyword_args))

  return start_openai_stream(vm, config, options, callback_val)

# Native function: Stream chat completion (namespace/global)
proc vm_openai_stream*(vm: VirtualMachine, args: ptr UncheckedArray[Value], arg_count: int, has_keyword_args: bool): Value {.gcsafe.} =
  if get_positional_count(arg_count, has_keyword_args) < 3:
    return new_error("OpenAI stream requires client, options, and handler arguments")

  let client_val = get_positional_arg(args, 0, has_keyword_args)
  let options_val = get_positional_arg(args, 1, has_keyword_args)
  let handler_val = get_positional_arg(args, 2, has_keyword_args)

  let (config, err) = fetch_client_config(client_val)
  if err != NIL:
    return err

  let options = geneValueToJson(options_val)
  return start_openai_stream(vm, config, options, handler_val)

# Native function: Chat completion as instance method
proc vm_openai_client_chat*(vm: VirtualMachine, args: ptr UncheckedArray[Value], arg_count: int, has_keyword_args: bool): Value {.gcsafe.} =
  if get_positional_count(arg_count, has_keyword_args) < 1:
    return new_error("OpenAI client chat requires self argument")

  let self_val = get_positional_arg(args, 0, has_keyword_args)
  let (config, err) = fetch_client_config(self_val)
  if err != NIL:
    return err

  var options = %*{}
  if get_positional_count(arg_count, has_keyword_args) > 1:
    options = geneValueToJson(get_positional_arg(args, 1, has_keyword_args))

  let payload = buildChatPayload(config, options)
  return call_openai_endpoint(config, "/chat/completions", payload)

proc vm_openai_client_embeddings*(vm: VirtualMachine, args: ptr UncheckedArray[Value], arg_count: int, has_keyword_args: bool): Value {.gcsafe.} =
  if get_positional_count(arg_count, has_keyword_args) < 1:
    return new_error("OpenAI client embeddings requires self argument")

  let self_val = get_positional_arg(args, 0, has_keyword_args)
  let (config, err) = fetch_client_config(self_val)
  if err != NIL:
    return err

  var options = %*{}
  if get_positional_count(arg_count, has_keyword_args) > 1:
    options = geneValueToJson(get_positional_arg(args, 1, has_keyword_args))

  let payload = buildEmbeddingsPayload(config, options)
  return call_openai_endpoint(config, "/embeddings", payload)

proc vm_openai_client_respond*(vm: VirtualMachine, args: ptr UncheckedArray[Value], arg_count: int, has_keyword_args: bool): Value {.gcsafe.} =
  if get_positional_count(arg_count, has_keyword_args) < 1:
    return new_error("OpenAI client respond requires self argument")

  let self_val = get_positional_arg(args, 0, has_keyword_args)
  let (config, err) = fetch_client_config(self_val)
  if err != NIL:
    return err

  var options = %*{}
  if get_positional_count(arg_count, has_keyword_args) > 1:
    options = geneValueToJson(get_positional_arg(args, 1, has_keyword_args))

  let payload = buildResponsesPayload(config, options)
  return call_openai_endpoint(config, "/responses", payload)

proc attach_openai_client_class*(cls: Class) =
  openai_client_class = cls
  cls.def_native_constructor(openai_client_constructor)
  cls.def_native_method("chat", vm_openai_client_chat)
  cls.def_native_method("embeddings", vm_openai_client_embeddings)
  cls.def_native_method("respond", vm_openai_client_respond)
  cls.def_native_method("stream", vm_openai_client_stream)
  if openai_error_class.isNil:
    var parent_cls: Class = nil
    if App.app.object_class.kind == VkClass:
      parent_cls = App.app.object_class.ref.class
    openai_error_class = new_class("OpenAIError", parent_cls)
    openai_error_class.def_native_method("to_s", openai_error_to_s)

# Initialize OpenAI classes and functions
proc init_openai_classes*() =
  VmCreatedCallbacks.add proc() {.gcsafe.} =
    if App == NIL or App.kind != VkApplication:
      return
    if App.app.global_ns == NIL or App.app.global_ns.kind != VkNamespace:
      return
    if App.app.genex_ns == NIL or App.app.genex_ns.kind != VkNamespace:
      return

    # Create OpenAI namespace
    let ai_ns = new_namespace("ai")

    # Create OpenAIClient class and attach native constructor/methods
    var base_parent: Class = nil
    if App.app.object_class.kind == VkClass:
      base_parent = App.app.object_class.ref.class
    let openai_client_class = new_class("OpenAIClient", base_parent)
    {.cast(gcsafe).}:
      attach_openai_client_class(openai_client_class)

    # Register OpenAI client constructor helper
    let global_ns = App.app.global_ns.ref.ns
    ai_ns["new_client".to_key()] = vm_openai_new_client.to_value()
    global_ns["openai_new_client".to_key()] = vm_openai_new_client.to_value()

    # Register the class in namespaces
    let openai_class_ref = new_ref(VkClass)
    openai_class_ref.class = openai_client_class
    let openai_class_value = openai_class_ref.to_ref_value()

    ai_ns["OpenAIClient".to_key()] = openai_class_value
    global_ns["OpenAIClient".to_key()] = openai_class_value

    {.cast(gcsafe).}:
      if not openai_error_class.isNil:
        let error_class_ref = new_ref(VkClass)
        error_class_ref.class = openai_error_class
        let error_class_value = error_class_ref.to_ref_value()
        ai_ns["OpenAIError".to_key()] = error_class_value
        global_ns["OpenAIError".to_key()] = error_class_value

    # Register the AI namespace in genex namespace
    App.app.genex_ns.ref.ns["ai".to_key()] = ai_ns.to_value()

    # Register convenience functions in ai namespace for direct access
    ai_ns["chat".to_key()] = vm_openai_chat.to_value()
    ai_ns["embeddings".to_key()] = vm_openai_embeddings.to_value()
    ai_ns["respond".to_key()] = vm_openai_respond.to_value()
    ai_ns["stream".to_key()] = vm_openai_stream.to_value()

# Call init function
init_openai_classes()
