#!/usr/bin/env gene
# LLM Inference Test for Gene + llama.cpp on Apple Silicon
# Tests the complete Model â†’ Session â†’ infer workflow

(println "ğŸ Gene LLM Test - Apple Silicon")
(println "=================================")

# Load the TinyLlama model
(println "ğŸ“¥ Loading TinyLlama model...")
(var model_path "tmp/models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf")
(var model (load_model model_path))

(println "âœ… Model loaded successfully!")
(println "   Model path: " model_path)
(println "   Model object: " model)

# Create a session with inference parameters
(println "")
(println "ğŸ”§ Creating inference session...")
(var session (model.new_session {:context_len 2048 :temperature 0.7 :top_p 0.9 :top_k 40 :seed 42 :max_tokens 100}))

(println "âœ… Session created successfully!")
(println "   Session object: " session)

# Test basic inference
(println "")
(println "ğŸš€ Testing basic inference...")
(var prompt "Hello! My name is")
(println "   Prompt: \"" prompt "\"")

(var start_time (timestamp))
(var result (session.infer prompt))
(var end_time (timestamp))

(var latency_ms (- end_time start_time))

(println "âœ… Inference completed!")
(println "   Response: \"" result.text "\"")
(println "   Tokens: " result/tokens)
(println "   Finish reason: " result/finish_reason)
(println "   Latency: " latency_ms "ms")

# Test a second inference to test conversation context
(println "")
(println "ğŸš€ Testing contextual inference...")
(var prompt2 "What did I just tell you my name was?")
(println "   Prompt: \"" prompt2 "\"")

(var start_time2 (timestamp))
(var result2 (session .infer prompt2))
(var end_time2 (timestamp))

(var latency_ms2 (- end_time2 start_time2))

(println "âœ… Contextual inference completed!")
(println "   Response: \"" result2/text "\"")
(println "   Tokens: " result2/tokens)
(println "   Finish reason: " result2/finish_reason)
(println "   Latency: " latency_ms2 "ms")

# Test error handling with invalid model path
(println "")
(println "ğŸ§ª Testing error handling...")
(try
  (var invalid_model (load_model "tmp/models/nonexistent.gguf"))
  (println "âŒ Should have failed!")
catch *
  (println "âœ… Error handling working: " $ex/.message)
)

# Cleanup resources
(println "")
(println "ğŸ§¹ Cleaning up resources...")
(session .close)
(model .close)

(println "âœ… Resources cleaned up!")
(println "")
(println "ğŸ‰ LLM test completed successfully!")
(println "")
(println "ğŸ“Š Performance Summary:")
(println "   First inference: " latency_ms "ms")
(println "   Second inference: " latency_ms2 "ms")
(println "   Average latency: " (/ (+ latency_ms latency_ms2) 2) "ms")

(println "")
(println "ğŸ Apple Silicon optimizations active:")
(println "   âœ… Metal acceleration")
(println "   âœ… ARM64 native instructions")
(println "   âœ… Matrix multiplication acceleration")