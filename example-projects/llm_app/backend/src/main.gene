#!/usr/bin/env gene run

# LLM Chat Backend Server
# Provides REST API endpoints for chat with a local LLM

# Configuration
(var PORT 3000)
(var MODEL_PATH ($env .get "GENE_LLM_MODEL" ""))
(var model NIL)
(var session NIL)

# Initialize LLM if model path is configured
(fn init_llm []
  (if (MODEL_PATH != "")
    (do
      (println "Loading LLM model from:" MODEL_PATH)
      (try
        (model = (genex/llm/load_model MODEL_PATH))
        (session = (model .new_session {^max_tokens 512}))
        (println "LLM model loaded successfully")
      catch *
        (println "Failed to load LLM model:" ($ex .message))
        (model = NIL)
        (session = NIL)
      )
    )
  else
    (println "No GENE_LLM_MODEL environment variable set - running in mock mode")
  )
)

# Quote character for JSON building
(var Q "\"")

# Escape string for JSON
(fn escape_json [s]
  (if (s == NIL)
    (return "null")
  )
  (var result (s .replace "\\" "\\\\"))
  (result = (result .replace Q "\\\""))
  (result = (result .replace "\n" "\\n"))
  (result = (result .replace "\r" "\\r"))
  (result = (result .replace "\t" "\\t"))
  #"#{Q}#{result}#{Q}"
)

# Strip <think>...</think> tags from response, return [thinking, answer]
(fn parse_response [text]
  (var think_start (text .index "<think>"))
  (if (think_start < 0)
    (return [NIL (text .trim)])
  )
  (var think_end (text .index "</think>"))
  (if (think_end < 0)
    (return [NIL (text .trim)])
  )
  # substr takes (start, length), not (start, end)
  (var think_len ((think_end - think_start) - 7))
  (var thinking (text .substr (think_start + 7) think_len))
  (var answer (text .substr (think_end + 8)))
  [(thinking .trim) (answer .trim)]
)

# Health check endpoint
(fn handle_health [req]
  (var is_loaded (model != NIL))
  (var loaded_str "false")
  (if is_loaded
    (loaded_str = "true")
  )
  (var body #"{#{Q}status#{Q}:#{Q}ok#{Q},#{Q}model_loaded#{Q}:#{loaded_str}}")
  (respond 200 body {^Content-Type "application/json"})
)

# Chat endpoint
(fn handle_chat [req]
  (var req_body req/body)

  # Parse JSON body
  (var parsed NIL)
  (try
    (parsed = (gene/json/parse req_body))
  catch *
    (var err_body #"{#{Q}error#{Q}:#{Q}Invalid JSON#{Q}}")
    (return (respond 400 err_body {^Content-Type "application/json"}))
  )

  # Extract message
  (var message (parsed .get "message"))
  (if (message == NIL)
    (var err_body #"{#{Q}error#{Q}:#{Q}Missing 'message' field#{Q}}")
    (return (respond 400 err_body {^Content-Type "application/json"}))
  )

  # Generate response
  (var response_text "")
  (var tokens_used 0)
  (var thinking NIL)

  (if (session != NIL)
    (do
      # Use actual LLM with chat format
      (var prompt #"<|im_start|>user\n#{message}\n<|im_end|>\n<|im_start|>assistant\n")
      (var result (session .infer prompt))
      (var raw_text (result .get "text"))
      (var tokens (result .get "tokens"))
      (tokens_used = (tokens .size))
      # Parse thinking vs answer
      (var parsed_resp (parse_response raw_text))
      (thinking = (parsed_resp .get 0))
      (response_text = (parsed_resp .get 1))
    )
  else
    (do
      # Mock response when no model is loaded
      (response_text = "I'm a mock response. Set GENE_LLM_MODEL environment variable to use a real LLM.")
      (tokens_used = 15)
    )
  )

  # Build JSON response
  (var think_json (escape_json thinking))
  (var resp_json (escape_json response_text))
  (var resp_body #"{#{Q}response#{Q}:#{resp_json},#{Q}thinking#{Q}:#{think_json},#{Q}tokens_used#{Q}:#{tokens_used}}")
  (respond 200 resp_body {^Content-Type "application/json"})
)

# Main request router
(fn handle_request [req]
  (var path req/path)
  (var method req/method)

  (println method path)

  # Route requests
  (if (path == "/api/health")
    (handle_health req)
  elif ((path == "/api/chat") && (method == "POST"))
    (handle_chat req)
  else
    (var err_body #"{#{Q}error#{Q}:#{Q}Not found#{Q}}")
    (respond 404 err_body {^Content-Type "application/json"})
  )
)

# Start server
(println "=== LLM Chat Backend ===")
(init_llm)
(println "Starting server on http://localhost:" PORT)
(println "Endpoints:")
(println "  GET  /api/health - Health check")
(println "  POST /api/chat   - Send chat message")
(println)

(start_server PORT handle_request)
(run_forever)
