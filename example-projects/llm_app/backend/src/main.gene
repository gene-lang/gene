#!/usr/bin/env gene run

# LLM Chat Backend Server
# Provides REST API endpoints for chat with a local LLM

# Configuration
(var PORT 4080)
(var MODEL_PATH ($env .get "GENE_LLM_MODEL" ""))

# Initialize LLM if model path is configured
# The model is registered globally so worker threads can access it
(fn init_llm []
  (var model_path_len (if (MODEL_PATH == nil) 0 else (MODEL_PATH .length)))
  (if (model_path_len == 0)
    (println "No GENE_LLM_MODEL environment variable set - running in mock mode")
  elif genex/llm
    (println "Loading LLM model from:" MODEL_PATH)
    (var model (genex/llm/load_model MODEL_PATH))
    # Register model globally for worker thread access
    (genex/llm/register_model model)
    (println "LLM model loaded and registered successfully")
  else
    (println "GENE_LLM_MODEL is set, but genex/llm is unavailable. Rebuild with -d:geneLLM.")
  )
)

# JSON response helper
(fn json_response [status data]
  (respond status data/.to_json {^Content-Type "application/json"})
)

# Strip <think>...</think> tags from response, return [thinking, answer]
(fn parse_response [text]
  (var think_start (text .index "<think>"))
  (if (think_start < 0)
    (return [nil text/.trim])
  )
  (var think_end (text .index "</think>"))
  (if (think_end < 0)
    (return [nil text/.trim])
  )
  # substr takes (start, length), not (start, end)
  (var think_len ((think_end - think_start) - 7))
  (var thinking (text .substr (think_start + 7) think_len))
  (var answer (text .substr (think_end + 8)))
  [thinking/.trim answer/.trim]
)

# Health check endpoint
(fn handle_health [req]
  (var is_loaded ((genex/llm/get_model) != nil))
  (json_response 200 {^status "ok" ^model_loaded is_loaded})
)

# Chat endpoint
(fn handle_chat [req]
(var req_body req/.body)
(var body_str (if (req_body == nil) "" else req_body))
(var content_type ((req/.headers .get "content-type" "") .to_lower))
  (var message)
  (var document_text)
  (var document_chunks 0)
  (var document_used false)

  (if ((content_type .index "multipart/form-data") >= 0)
    (try
      (ai/documents/validate_upload req {^allowed_types ["pdf" "png" "jpg" "jpeg" "bmp" "tiff" "tif"]})
    catch *
      (return (json_response 400 {^error $ex/message}))
    )
    (try
      (document_text = (ai/documents/extract_upload req))
    catch *
      (return (json_response 400 {^error $ex/message}))
    )
    (message = ((req/.params .get "message" "Summarize the document.") .trim))
    (document_used = true)
  else
    (var trimmed_body (body_str .trim))
    (if ((trimmed_body .length) == 0)
      (message = (req/.params .get "message"))
      (if ((message == nil) || ((message .length) == 0))
        (return (json_response 400 {^error "Missing 'message' field"}))
      )
    else
      # Parse JSON body
      (var parsed)
      (try
        (parsed = (gene/json/parse req_body))
      catch *
        (return (json_response 400 {^error "Invalid JSON"}))
      )

      # Extract message
      (message = (parsed .get "message"))
      (if (message == nil)
        (return (json_response 400 {^error "Missing 'message' field"}))
      )
    )
  )

  (if (document_text != nil)
    (var chunks (ai/documents/chunk document_text {^strategy :recursive ^size 800 ^overlap 80}))
    (document_chunks = (chunks .size))
    (var context_parts [])
    (var i 0)
    (var max_chunks 3)
    (loop
      (if ((i >= (chunks .size)) || (i >= max_chunks))
        (break)
      )
      (var chunk (chunks .get i))
      (if (chunk != nil)
        (context_parts .append chunk/text)
      )
      (i += 1)
    )
    (var context (context_parts .join "\n\n"))
    (message = #"""
You are given extracted document context.

Context:
#{context}

User request:
#{message}
""")
  )

  # Generate response
  (var response_text "")
  (var tokens_used 0)
  (var thinking)

  # Get model from global registry (works across threads)
  (var model (genex/llm/get_model))
  
  (if model
    # Create session on-demand for this request
    (var session (model .new_session {^max_tokens 512}))
    (try
      # Use actual LLM with chat format
      (var prompt #"<|im_start|>user\n#{message}\n<|im_end|>\n<|im_start|>assistant\n")
      (var result (session .infer prompt))
      (var raw_text (result .get "text"))
      (var tokens (result .get "tokens"))
      (tokens_used = tokens/.size)
      # Parse thinking vs answer
      (var parsed_resp (parse_response raw_text))
      (thinking = (parsed_resp .get 0))
      (response_text = (parsed_resp .get 1))
    finally
      # Always close session after request
      (session .close)
    )
  else
    # Mock response when no model is loaded
    (response_text = "I'm a mock response. Set GENE_LLM_MODEL environment variable to use a real LLM.")
    (tokens_used = 15)
  )

  (json_response 200 {^response response_text ^thinking thinking ^tokens_used tokens_used ^document_used document_used ^document_chunks document_chunks})
)

# Main request router
(fn handle_request [req]
  (var path req/.path)
  (var method req/.method)

  (println method path)

  # Route requests
  (if (path == "/api/health")
    (handle_health req)
  elif ((path == "/api/chat") && ((method == "POST") || (method == "GET")))
    (handle_chat req)
  else
    (json_response 404 {^error "Not found"})
  )
)

# Start server
(println "=== LLM Chat Backend ===")
(init_llm)
(println "Starting server on http://localhost:" PORT)
(println "Endpoints:")
(println "  GET  /api/health - Health check")
(println "  POST /api/chat   - Send chat message")
(println)

# Use concurrent mode with thread-safe model registry access
# Model is protected by lock, sessions are created per-request
(start_server PORT handle_request ^concurrent true ^workers 4)
(run_forever)
