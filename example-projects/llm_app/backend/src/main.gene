#!/usr/bin/env gene run

# LLM Chat Backend Server
# Provides REST API endpoints for chat with a local LLM

# Configuration
(var PORT 4080)
(var MODEL_PATH ($env .get "GENE_LLM_MODEL" ""))
(var DB_PATH "chat.sqlite")
(var DB nil)

# ============================================================
# Tool Registry - Functions that LLM can call
# ============================================================

(var TOOLS {})

# Register a tool
(fn register_tool [name description params handler]
  (TOOLS .set name {
    ^description description
    ^params params
    ^handler handler
  })
)

# Built-in tools
(register_tool "get_time"
  "Get the current date and time"
  {}
  (fn [args]
    {^result (time/now/.to_s)}
  )
)

(register_tool "calculate"
  "Evaluate a mathematical expression. Use this for any math calculations."
  {^expression "The math expression to evaluate (e.g., '2 + 2', '10 * 5')"}
  (fn [args]
    (var expr (args .get "expression" ""))
    (try
      (var result (eval (parse expr)))
      {^result result/.to_s}
    catch *
      {^error #"Failed to evaluate: #{expr}"}
    )
  )
)

(register_tool "get_weather"
  "Get the current weather for a location"
  {^location "The city or location to get weather for"}
  (fn [args]
    (var location (args .get "location" "Unknown"))
    # Mock weather data - in production, call a real API
    {^result #"Weather in #{location}: 72°F (22°C), Partly Cloudy. Humidity: 45%"}
  )
)

# Build system prompt with tool descriptions
(fn build_system_prompt []
  (var tool_list [])
  (TOOLS .each (fn [name tool]
    (var params_desc "")
    (if ((tool/params .keys) .size > 0)
      (var param_parts [])
      (tool/params .each (fn [pname pdesc]
        (param_parts .append (new String "    " pname ": " pdesc))
      ))
      (params_desc = (param_parts .join "\n"))
    )
    (if (params_desc/.length > 0)
      (tool_list .append (new String "- " name ": " tool/description "\n  Parameters:\n" params_desc))
    else
      (tool_list .append (new String "- " name ": " tool/description))
    )
  ))
  (var tools_section (tool_list .join "\n"))
  (new String "You are a helpful AI assistant with access to tools.\n\n"
       "Available tools:\n" tools_section "\n\n"
       "When you need to use a tool, respond with ONLY a JSON object in this exact format:\n"
       "{\"tool\": \"tool_name\", \"args\": {\"param\": \"value\"}}\n\n"
       "After receiving a tool result, provide a natural response to the user.\n"
       "Only use tools when necessary. For general conversation, respond normally.")
)

# Parse tool call from LLM response
(fn parse_tool_call [text]
  # Look for JSON object with tool field
  (var trimmed text/.trim)
  (if (!(trimmed .starts_with "{"))
    (return nil)
  )
  (try
    (var parsed trimmed/.from_json)
    (if (parsed .has "tool")
      parsed
    else
      nil
    )
  catch *
    nil
  )
)

# Execute a tool by name
(fn execute_tool [name args]
  (var tool (TOOLS .get name))
  (if (tool == nil)
    (return {^error #"Unknown tool: #{name}"})
  )
  (try
    ((tool/handler) args)
  catch *
    {^error #"Tool execution failed: #{$ex/message}"}
  )
)

# Initialize LLM if model path is configured
# The model is registered globally so worker threads can access it
(fn init_llm []
  (if (MODEL_PATH/.length == 0)
    (println "No GENE_LLM_MODEL environment variable set - running in mock mode")
  elif genex/llm
    (println "Loading LLM model from:" MODEL_PATH)
    # Adjust layers for smaller models or GPUs with more VRAM
    (var model (genex/llm/load_model MODEL_PATH {^gpu_layers 99}))
    # Register model globally for worker thread access
    (genex/llm/register_model model)
    (println "LLM model loaded and registered successfully")
  else
    (println "GENE_LLM_MODEL is set, but genex/llm is unavailable. Rebuild with -d:geneLLM.")
  )
)

(fn init_db []
  (if genex/sqlite
    (try
      (DB = (genex/sqlite/open DB_PATH))
      (DB .exec "PRAGMA journal_mode=WAL")
      (DB .exec "CREATE TABLE IF NOT EXISTS conversations (id INTEGER PRIMARY KEY AUTOINCREMENT, created_at REAL NOT NULL)")
      (DB .exec "CREATE TABLE IF NOT EXISTS messages (id INTEGER PRIMARY KEY AUTOINCREMENT, conversation_id INTEGER NOT NULL, role TEXT NOT NULL, content TEXT NOT NULL, document_used INTEGER DEFAULT 0, document_chunks INTEGER DEFAULT 0, created_at REAL NOT NULL, FOREIGN KEY(conversation_id) REFERENCES conversations(id))")
      (DB .exec "CREATE INDEX IF NOT EXISTS idx_messages_conversation_id ON messages(conversation_id, id)")
    catch *
      (println "Failed to open SQLite:" $ex/message)
      (DB = nil)
    )
  else
    (println "genex/sqlite is unavailable. Rebuild with extensions.")
  )
)

# JSON response helper
(fn json_response [status data]
  (respond status data/.to_json {^Content-Type "application/json"})
)

# Strip <think>...</think> tags from response, return [thinking, answer]
(fn parse_response [text]
  (var think_start (text .index "<think>"))
  (if (think_start < 0)
    (return [nil text/.trim])
  )
  (var think_end (text .index "</think>"))
  (if (think_end < 0)
    (return [nil text/.trim])
  )
  # substr takes (start, length), not (start, end)
  (var think_len ((think_end - think_start) - 7))
  (var thinking (text .substr (think_start + 7) think_len))
  (var answer (text .substr (think_end + 8)))
  [thinking/.trim answer/.trim]
)

(fn new_conversation []
  (if (DB == nil)
    (return nil)
  )
  (DB .exec "INSERT INTO conversations (created_at) VALUES (?)" (time/now))
  (var rows (DB .query "SELECT last_insert_rowid()"))
  (if ((rows .size) == 0)
    (return nil)
  )
  (var id_val rows/0/0)
  id_val/.to_s
)

(fn conversation_exists [conv_id_int]
  (if (DB == nil)
    (return false)
  )
  (var rows (DB .query "SELECT id FROM conversations WHERE id = ?" conv_id_int))
  ((rows .size) > 0)
)

(fn load_history [conv_id_int]
  (if (DB == nil)
    (return [])
  )
  (var rows (DB .query "SELECT role, content FROM messages WHERE conversation_id = ? ORDER BY id" conv_id_int))
  (var history [])
  (var i 0)
  (loop
    (if (i >= rows/.size)
      (break)
    )
    (var row (rows .get i))
    (history .append {^role row/0 ^content row/1})
    (i += 1)
  )
  history
)

(fn store_message [conv_id_int role content document_used document_chunks]
  (if (DB == nil)
    (return nil)
  )
  (var doc_flag (if document_used 1 else 0))
  (var chunks_val (if document_used document_chunks else 0))
  (DB .exec "INSERT INTO messages (conversation_id, role, content, document_used, document_chunks, created_at) VALUES (?, ?, ?, ?, ?, ?)"
    conv_id_int
    role
    content
    doc_flag
    chunks_val
    (time/now)
  )
)

(fn build_prompt [history message]
  (var parts [])
  # Add system prompt with tool descriptions
  (var system_prompt (build_system_prompt))
  (parts .append #"<|im_start|>system\n#{system_prompt}\n<|im_end|>\n")
  (var i 0)
  (loop
    (if (i >= (history .size))
      (break)
    )
    (var entry (history .get i))
    (var role entry/role)
    (var content entry/content)
    (if (role == "user")
      (parts .append #"<|im_start|>user\n#{content}\n<|im_end|>\n")
    elif (role == "assistant")
      (parts .append #"<|im_start|>assistant\n#{content}\n<|im_end|>\n")
    elif (role == "tool_result")
      (parts .append #"<|im_start|>user\nTool result: #{content}\n<|im_end|>\n")
    )
    (i += 1)
  )
  (parts .append #"<|im_start|>user\n#{message}\n<|im_end|>\n<|im_start|>assistant\n")
  (parts .join "")
)

(fn chat_id_from_path [path]
  (var prefix "/api/chat/")
  (if (path .starts_with prefix)
    (return (path .substr prefix/.length))
  )
  nil
)

# Health check endpoint
(fn handle_health [req]
  (var is_loaded
    (if genex/llm
      (if (genex/llm/get_model) true else false)
    else
      false
    )
  )
  (json_response 200 {^status "ok" ^model_loaded is_loaded})
)

(fn handle_chat_new [req]
  (if (DB == nil)
    (return (json_response 500 {^error "SQLite is unavailable"}))
  )
  (var conv_id (new_conversation))
  (if (conv_id == nil)
    (return (json_response 500 {^error "Failed to create conversation"}))
  )
  (json_response 200 {^conversation_id conv_id})
)

# Chat endpoint
(fn handle_chat [req conv_id]
  (if (DB == nil)
    (return (json_response 500 {^error "SQLite is unavailable"}))
  )
  (if (conv_id == nil)
    (return (json_response 400 {^error "Missing conversation id"}))
  )
  (var conv_id_int conv_id/.to_i)
  (if (conv_id_int <= 0)
    (return (json_response 400 {^error "Invalid conversation id"}))
  )
  (var history [])
  (if (conversation_exists conv_id_int)
    (history = (load_history conv_id_int))
  else
    (return (json_response 404 {^error "Conversation not found" ^conversation_id conv_id}))
  )
  (var req_body req/.body)
  (var body_str (if (req_body == nil) "" else req_body))
  (var content_type ((req/.headers .get "content-type" "") .to_lower))
  (var message)
  (var document_text)
  (var document_chunks 0)
  (var document_used false)

  (if ((content_type .index "multipart/form-data") >= 0)
    (try
      (ai/documents/validate_upload req {^allowed_types ["pdf" "png" "jpg" "jpeg" "bmp" "tiff" "tif"]})
    catch *
      (return (json_response 400 {^error $ex/message}))
    )
    (try
      (document_text = (ai/documents/extract_upload req))
    catch *
      (return (json_response 400 {^error $ex/message}))
    )
    (message = ((req/.params .get "message" "Summarize the document.") .trim))
    (document_used = true)
  else
    (var trimmed_body (body_str .trim))
    (if ((trimmed_body .length) == 0)
      (message = (req/.params .get "message"))
      (if ((message == nil) || ((message .length) == 0))
        (return (json_response 400 {^error "Missing 'message' field"}))
      )
    else
      # Parse JSON body
      (var parsed)
      (try
        (parsed = (gene/json/parse req_body))
      catch *
        (return (json_response 400 {^error "Invalid JSON"}))
      )

      # Extract message
      (message = (parsed .get "message"))
      (if (message == nil)
        (return (json_response 400 {^error "Missing 'message' field"}))
      )
    )
  )

  (if (document_text != nil)
    (var chunks (ai/documents/chunk document_text {^strategy :recursive ^size 800 ^overlap 80}))
    (document_chunks = chunks/.size)
    (var context_parts [])
    (var i 0)
    (var max_chunks 3)
    (loop
      (if ((i >= chunks/.size) || (i >= max_chunks))
        (break)
      )
      (var chunk (chunks .get i))
      (if (chunk != nil)
        (context_parts .append chunk/text)
      )
      (i += 1)
    )
    (var context (context_parts .join "\n\n"))
    (message = #"""
You are given extracted document context.

Context:
#{context}

User request:
#{message}
""")
  )
  (var prompt_message message)

  # Generate response with tool execution loop
  (var response_text "")
  (var tokens_used 0)
  (var thinking)
  (var tool_calls [])

  # Get model from global registry (works across threads)
  (var model (if genex/llm (genex/llm/get_model) else false))

  (if model
    # Create session on-demand for this request
    (var session (model .new_session {^max_tokens 512}))
    (try
      # Build conversation with history
      (var current_history history)
      (var current_message prompt_message)
      (var max_tool_iterations 5)
      (var iteration 0)
      
      (loop
        (if (iteration >= max_tool_iterations)
          (response_text = "I apologize, but I'm having trouble completing this request after multiple attempts.")
          (break)
        )
        
        # Build prompt and generate response
        (var prompt (build_prompt current_history current_message))
        (var result (session .infer prompt))
        (var raw_text (result .get "text"))
        (var tokens (result .get "tokens"))
        (tokens_used += tokens/.size)
        
        # Check if this is a tool call
        (var tool_call (parse_tool_call raw_text))
        
        (if (tool_call != nil)
          # Execute the tool
          (var tool_name tool_call/tool)
          (var tool_args (tool_call .get "args" {}))
          (println #"Tool call: #{tool_name} with args: #{tool_args/.to_json}")
          
          (var tool_result (execute_tool tool_name tool_args))
          (tool_calls .append {^tool tool_name ^args tool_args ^result tool_result})
          
          # Add tool call and result to history for next iteration
          (current_history .append {^role "assistant" ^content raw_text})
          (current_message = (tool_result .get "result" (tool_result .get "error" "Tool returned no result")))
          (current_history .append {^role "tool_result" ^content current_message})
          
          (iteration += 1)
        else
          # No tool call - this is the final response
          (var parsed_resp (parse_response raw_text))
          (thinking = (parsed_resp .get 0))
          (response_text = (parsed_resp .get 1))
          (break)
        )
      )
    catch *
      (response_text = $ex/message)
    )
    # Note: session cleanup is handled automatically by LLM module's cleanup handler
    # Explicit close is not possible here due to VM scope bug (scope.members truncated in finally blocks)
  else
    # Mock response when no model is loaded
    (response_text = "I'm a mock response. Set GENE_LLM_MODEL environment variable to use a real LLM.")
    (tokens_used = 15)
  )

  (store_message conv_id_int "user" prompt_message document_used document_chunks)
  (store_message conv_id_int "assistant" response_text false 0)

  (var response_data {
    ^conversation_id conv_id
    ^response response_text
    ^thinking thinking
    ^tokens_used tokens_used
    ^document_used document_used
    ^document_chunks document_chunks
  })
  (if (tool_calls/.size > 0)
    (response_data .set "tool_calls" tool_calls)
  )
  (json_response 200 response_data)
)

# Main request router
(fn handle_request [req]
  (var path req/.path)
  (var method req/.method)

  (println method path)

  # Route requests
  (if (path == "/api/health")
    (handle_health req)
  elif ((path == "/api/chat/new") && (method == "POST"))
    (handle_chat_new req)
  elif ((path .starts_with "/api/chat/") && (method == "POST"))
    (var conv_id (chat_id_from_path path))
    (handle_chat req conv_id)
  else
    (json_response 404 {^error "Not found"})
  )
)

# Start server
(println "=== LLM Chat Backend ===")
(init_db)
(init_llm)
(println "Starting server on http://localhost:" PORT)
(println "Endpoints:")
(println "  GET  /api/health - Health check")
(println "  POST /api/chat/new - Start conversation")
(println "  POST /api/chat/{id} - Send chat message")
(println)

# (start_server PORT handle_request ^concurrent true ^workers 4)

# LLM apps must run in sequential mode because:
# 1. Nim's ref objects (ModelState, SessionState) are GC-managed and thread-local
# 2. Sharing these across threads causes segfaults
# 3. llama.cpp operations are also not thread-safe
# For non-LLM HTTP servers, concurrent mode works fine.
(start_server PORT handle_request)

(run_forever)
